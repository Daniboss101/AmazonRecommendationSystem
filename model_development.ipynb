{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:40:25.868839Z",
     "start_time": "2025-09-22T22:38:47.731377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "parquet_file = pq.ParquetFile('amazon_df.parquet')\n",
    "chunk_size = 100000\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "    chunk_df = batch.to_pandas()\n",
    "    print(f\"Chunk shape: {chunk_df.shape}\")\n",
    "    chunks.append(chunk_df)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(f\"Total interactions: {len(df)}\")\n",
    "print(f\"Unique users: {len(set(df['user_id']))}\")\n",
    "print(f\"Unique items: {len(set(df['parent_asin']))}\")\n",
    "print(f\"Rating Distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ],
   "id": "4a08b12daa291e7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (100000, 28)\n",
      "Chunk shape: (83400, 28)\n",
      "Total interactions: 2483400\n",
      "Unique users: 316045\n",
      "Unique items: 1300847\n",
      "Rating Distribution:\n",
      "rating\n",
      "1.0     174371\n",
      "2.0      98312\n",
      "3.0     180027\n",
      "4.0     336999\n",
      "5.0    1693691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:42:32.161162Z",
     "start_time": "2025-09-22T22:42:27.767746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def create_interaction_matrix(df, user_col='user_id', item_col='parent_asin', rating_col='rating'):\n",
    "    users = df[user_col].unique()\n",
    "    items = df[item_col].unique()\n",
    "    \n",
    "    user_to_idx = {user: idx for idx, user in enumerate(users)}\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(items)}\n",
    "    \n",
    "    idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "    idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "    \n",
    "    user_indices = df[user_col].map(user_to_idx)\n",
    "    item_indices = df[item_col].map(item_to_idx)\n",
    "    ratings = df[rating_col].values\n",
    "    \n",
    "    interaction_matrix = csr_matrix((ratings, (user_indices, item_indices)), shape=(len(users), len(items)))\n",
    "    \n",
    "    return interaction_matrix, user_to_idx, item_to_idx, idx_to_user, idx_to_item\n",
    "\n",
    "interaction_matrix, user_to_idx, item_to_idx, idx_to_user, idx_to_item = create_interaction_matrix(df)\n",
    "print(f\"Matrix shape: {interaction_matrix.shape}\")\n",
    "print(f\"Matrix density: {interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1]) * 100:.4f}%\")"
   ],
   "id": "6d8ff090ccf5be14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (316045, 1300847)\n",
      "Matrix density: 0.0006%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:43:04.123080Z",
     "start_time": "2025-09-22T22:42:36.015287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class MatrixFactorizationRecommender: \n",
    "    def __init__(self, n_factors=50):\n",
    "        self.n_factors = n_factors\n",
    "        self.svd =None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        \n",
    "    def fit(self, interaction_matrix): \n",
    "        self.svd = TruncatedSVD(n_components=self.n_factors, random_state=42)\n",
    "        \n",
    "        self.user_factors = self.svd.fit_transform(interaction_matrix)\n",
    "        self.item_factors = self.svd.components_.T\n",
    "        \n",
    "        print(f\"Trained SVD with {self.n_factors} factors\")\n",
    "        print(f\"User factors shape: {self.user_factors.shape}\")\n",
    "        print(f\"Item factors shape: {self.item_factors.shape}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "mf_model = MatrixFactorizationRecommender(n_factors=50)\n",
    "mf_model.fit(interaction_matrix)\n"
   ],
   "id": "16a2d0d7a64f663e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained SVD with 50 factors\n",
      "User factors shape: (316045, 50)\n",
      "Item factors shape: (1300847, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MatrixFactorizationRecommender at 0x22b7d549390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:43:16.114884Z",
     "start_time": "2025-09-22T22:43:16.099357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_user_recommendations(user_id, mf_model, user_to_idx, idx_to_item, interaction_matrix, n_recommendations=10):\n",
    "    if user_id not in user_to_idx:\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    \n",
    "    predicted_ratings = mf_model.user_factors[user_idx] @ mf_model.item_factors.T\n",
    "    \n",
    "    rated_items = set(interaction_matrix[user_idx].nonzero()[1])\n",
    "    \n",
    "    item_scores = []\n",
    "    for item_idx in range(len(predicted_ratings)):\n",
    "        if item_idx not in rated_items:\n",
    "            item_id = idx_to_item[item_idx]\n",
    "            score = predicted_ratings[item_idx]\n",
    "            item_scores.append((item_id, score))  \n",
    "            \n",
    "    item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return item_scores[:n_recommendations]"
   ],
   "id": "346ba5afa968d2b1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:43:21.259515Z",
     "start_time": "2025-09-22T22:43:21.236555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_smart_recommendations(user_id, df, mf_model, user_to_idx, idx_to_item, interaction_matrix, n_recommendations=10):\n",
    "    user_history = df[df['user_id'] == user_id]\n",
    "    \n",
    "    print(f\"User history categories: {user_history['main_category'].value_counts()}\")\n",
    "    primary_category = user_history['main_category'].mode().iloc[0] if len(user_history) > 0 else None\n",
    "    print(f\"Detected primary category: {primary_category}\")\n",
    "    \n",
    "    collab_recs = get_user_recommendations(user_id, mf_model, user_to_idx, idx_to_item, interaction_matrix, n_recommendations * 3)\n",
    "    \n",
    "    if collab_recs and collab_recs[0][1] > 0.5:  \n",
    "        return collab_recs[:n_recommendations]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Low collaborative confidence for {user_id}, using category fallback\")\n",
    "        \n",
    "        if len(user_history) > 0:\n",
    "            top_categories = user_history['main_category'].value_counts().head(3).index.tolist()\n",
    "            \n",
    "            recommendations = []\n",
    "            for category in top_categories:\n",
    "                category_items = df[df['main_category'] == category]\n",
    "                unbought_items = category_items[~category_items['parent_asin'].isin(user_history['parent_asin'])]\n",
    "                \n",
    "                if len(unbought_items) > 0:  \n",
    "                    item_popularity = unbought_items.groupby('parent_asin').agg({\n",
    "                        'rating': 'mean',\n",
    "                        'parent_asin': 'count'\n",
    "                    }).rename(columns={'parent_asin': 'review_count'})\n",
    "                    \n",
    "                    item_popularity['popularity_score'] = item_popularity['rating'] * np.log1p(item_popularity['review_count'])\n",
    "                    \n",
    "                    top_items = item_popularity.nlargest(n_recommendations//3, 'popularity_score')\n",
    "                    recommendations.extend([(item_id, score, category) for item_id, score in zip(top_items.index, top_items['popularity_score'])])\n",
    "            \n",
    "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            return recommendations[:n_recommendations]\n",
    "        \n",
    "        else:\n",
    "            return get_overall_popular_items(df, user_history, n_recommendations)\n",
    "\n",
    "def get_overall_popular_items(df, user_history, n_recommendations):\n",
    "    unbought_items = df[~df['parent_asin'].isin(user_history['parent_asin'])]\n",
    "    popular_items = unbought_items['parent_asin'].value_counts().head(n_recommendations)\n",
    "    return [(item_id, count) for item_id, count in popular_items.items()]"
   ],
   "id": "9e06f502264b5ec2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:44:04.820727Z",
     "start_time": "2025-09-22T22:43:47.775689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "sample_users = df['user_id'].value_counts().head(5).index.tolist()\n",
    "test_user = sample_users[0]\n",
    "\n",
    "print(f\"Testing recommendations for user: {test_user}\")\n",
    "\n",
    "user_history = df[df['user_id'] == test_user][['parent_asin', 'rating', 'main_category', 'title_meta']].head(10)\n",
    "print(f\"\\nUser's actual purchase history:\")\n",
    "print(user_history)\n",
    "\n",
    "recommendations = get_smart_recommendations(test_user, df, mf_model, user_to_idx, idx_to_item, interaction_matrix, n_recommendations=10)\n",
    "\n",
    "print(f\"\\nTop 10 recommendations:\")\n",
    "for i, recommendation in enumerate(recommendations, 1):\n",
    "    if len(recommendation) == 3:  \n",
    "        item_id, score, category = recommendation\n",
    "    else:  \n",
    "        item_id, score = recommendation\n",
    "        category = \"Unknown\"\n",
    "    \n",
    "    item_info = df[df['parent_asin'] == item_id].iloc[0]\n",
    "    print(f\"{i}. {item_info['title_meta'][:50]}... (Category: {item_info['main_category']}, Score: {score:.3f})\")"
   ],
   "id": "c45885e2e6a049ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing recommendations for user: AGZZXSMMS4WRHHJRBUJZI4FZDHKQ\n",
      "\n",
      "User's actual purchase history:\n",
      "    parent_asin  rating main_category  \\\n",
      "502  B081M5R5L5     5.0    All Beauty   \n",
      "633  B095H936ZB     5.0    All Beauty   \n",
      "634  B08CVTNQP1     5.0    All Beauty   \n",
      "635  B08JZD8HN4     5.0    All Beauty   \n",
      "636  B07Z548TKH     5.0    All Beauty   \n",
      "637  B07VQR3W3Z     5.0    All Beauty   \n",
      "638  B082MDFNZM     5.0    All Beauty   \n",
      "639  B082TSD9HN     5.0    All Beauty   \n",
      "640  B07XVNJFNF     5.0    All Beauty   \n",
      "641  B0815TNMTL     5.0    All Beauty   \n",
      "\n",
      "                                            title_meta  \n",
      "502  QIC 4D Silk Fiber Lash Mascara, Fiber Mascara,...  \n",
      "633  Sallcks Long Black Wavy Wigs for Women Long Cu...  \n",
      "634  Disney Frozen - Townley Girl Mega Nail Set wit...  \n",
      "635  12 Sheets 3D Iridescent Butterfly Nail Art Sti...  \n",
      "636  Face and Body Paint Kit By Color Technik 24 Pr...  \n",
      "637  Hair Scrunchies for women,Including 3 Pcs Long...  \n",
      "638  Baasha 4 Pcs Paddle Brush Set, Detangling Brus...  \n",
      "639  8 Pcs Unicorn Butterfly Hair Clips Alligator G...  \n",
      "640  100pcs Premium Quality Hair Clips, 20 Glossy C...  \n",
      "641  Headbands Hair Bands Wraps for Women, Boho Flo...  \n",
      "User history categories: main_category\n",
      "AMAZON FASHION               3498\n",
      "Toys & Games                 1676\n",
      "Amazon Home                  1267\n",
      "Office Products               619\n",
      "Tools & Home Improvement      496\n",
      "Arts, Crafts & Sewing         298\n",
      "All Electronics               283\n",
      "All Beauty                    252\n",
      "Books                         189\n",
      "Sports & Outdoors             173\n",
      "Industrial & Scientific       111\n",
      "Health & Personal Care         92\n",
      "Musical Instruments            80\n",
      "Computers                      74\n",
      "Pet Supplies                   58\n",
      "Cell Phones & Accessories      42\n",
      "Camera & Photo                 39\n",
      "Grocery                        36\n",
      "Automotive                     20\n",
      "Baby                           17\n",
      "                               17\n",
      "Handmade                       13\n",
      "Home Audio & Theater           10\n",
      "Video Games                    10\n",
      "Car Electronics                 9\n",
      "Premium Beauty                  6\n",
      "Software                        3\n",
      "Buy a Kindle                    2\n",
      "Appliances                      2\n",
      "Fine Art                        1\n",
      "Movies & TV                     1\n",
      "Name: count, dtype: int64\n",
      "Detected primary category: AMAZON FASHION\n",
      "Low collaborative confidence for AGZZXSMMS4WRHHJRBUJZI4FZDHKQ, using category fallback\n",
      "\n",
      "Top 10 recommendations:\n",
      "1. Funny Civil Engineers TShirt I'm A Crazy Civil Eng... (Category: AMAZON FASHION, Score: 29.271)\n",
      "2. DASH Rapid Egg Cooker: 6 Egg Capacity Electric Egg... (Category: Amazon Home, Score: 27.691)\n",
      "3. 12 Pack Keurig Filter Replacement by K&J - Compati... (Category: Amazon Home, Score: 27.138)\n",
      "4. Alvada Merino Wool Hiking Socks Thermal Warm Crew ... (Category: AMAZON FASHION, Score: 26.933)\n",
      "5. Linda's Essentials Silicone Stove Gap Covers (2 Pa... (Category: Amazon Home, Score: 25.581)\n",
      "6. XX-Large Slip Stop Single Tread Slipper Socks (6 P... (Category: AMAZON FASHION, Score: 23.602)\n",
      "7. LeapFrog Learning Friends English-Chinese 100 Word... (Category: Toys & Games, Score: 21.640)\n",
      "8. SHASHIBO Shape Shifting Box - Award-Winning, Paten... (Category: Toys & Games, Score: 20.824)\n",
      "9. Taco Cat Goat Cheese Pizza... (Category: Toys & Games, Score: 19.959)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Content Based Model**",
   "id": "429a5f14ec9bd04b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:44:10.871404Z",
     "start_time": "2025-09-22T22:44:10.800943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class FastContentBasedRecommender: \n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.content_features = None\n",
    "        self.item_to_idx = {}\n",
    "        self.idx_to_item = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.product_df = None\n",
    "        \n",
    "        self.item_similarities = {}  \n",
    "        self.similarity_computed = False\n",
    "        \n",
    "    def clean_text(self, text): \n",
    "        if pd.isna(text) or text == [] or text == '': \n",
    "            return ''\n",
    "        \n",
    "        if isinstance(text, list): \n",
    "            text = ' '.join(str(item) for item in text)\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        text = ''.join(c if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_product_features(self, df, max_products=None): \n",
    "        print(\"Extracting features for products...\")\n",
    "        print(f\"Total unique products in dataset: {df['parent_asin'].nunique()}\")\n",
    "        \n",
    "        if max_products:\n",
    "            print(f\"Limiting to {max_products} products for testing\")\n",
    "            unique_products = df['parent_asin'].unique()[:max_products]\n",
    "            df = df[df['parent_asin'].isin(unique_products)]\n",
    "        \n",
    "        product_features = []\n",
    "        \n",
    "        unique_asins = df['parent_asin'].unique()\n",
    "        total_products = len(unique_asins)\n",
    "        \n",
    "        for i, asin in enumerate(unique_asins):\n",
    "            if i % 50000 == 0:\n",
    "                print(f\"Processing product {i+1}/{total_products} ({100*i/total_products:.1f}%)\")\n",
    "            \n",
    "            group = df[df['parent_asin'] == asin]\n",
    "            \n",
    "            description = group['description'].iloc[0]\n",
    "            store = group['store'].iloc[0] if 'store' in group.columns else ''\n",
    "            price = group['price'].iloc[0] if 'price' in group.columns else None\n",
    "            main_category = group['main_category'].iloc[0] if 'main_category' in group.columns else ''\n",
    "            details = group['details'].iloc[0] if 'details' in group.columns else ''\n",
    "            \n",
    "            max_reviews = min(10, len(group))\n",
    "            review_subset = group.head(max_reviews)\n",
    "            \n",
    "            review_titles = ' '.join(review_subset['title_review'].fillna('').astype(str))\n",
    "            review_texts = ' '.join(review_subset['review_text'].fillna('').astype(str))\n",
    "            \n",
    "            review_texts = review_texts[:500] if len(review_texts) > 500 else review_texts\n",
    "            \n",
    "            avg_rating = group['rating'].mean()\n",
    "            num_reviews = len(group)\n",
    "            avg_helpful_votes = group['helpful_vote'].mean() if 'helpful_vote' in group.columns else 0\n",
    "            \n",
    "            product_features.append({\n",
    "                'parent_asin': asin, \n",
    "                'description': description, \n",
    "                'store': store, \n",
    "                'main_category': main_category, \n",
    "                'details': details, \n",
    "                'review_titles': review_titles, \n",
    "                'review_texts': review_texts, \n",
    "                'price': price, \n",
    "                'avg_rating': avg_rating, \n",
    "                'num_reviews': num_reviews, \n",
    "                'avg_helpful_votes': avg_helpful_votes\n",
    "            })\n",
    "            \n",
    "            if i % 100000 == 0:\n",
    "                gc.collect()\n",
    "                \n",
    "        return pd.DataFrame(product_features)\n",
    "    \n",
    "    def prepare_content_features(self, product_df): \n",
    "        print(\"Preparing content features...\")\n",
    "        \n",
    "        product_df['description_clean'] = product_df['description'].apply(self.clean_text)\n",
    "        product_df['store_clean'] = product_df['store'].apply(self.clean_text)\n",
    "        product_df['main_category_clean'] = product_df['main_category'].apply(self.clean_text)\n",
    "        product_df['details_clean'] = product_df['details'].apply(self.clean_text)\n",
    "        product_df['review_titles_clean'] = product_df['review_titles'].apply(self.clean_text)\n",
    "        product_df['review_texts_clean'] = product_df['review_texts'].apply(self.clean_text)\n",
    "        \n",
    "        product_df['combined_text'] = (\n",
    "            product_df['description_clean'] + ' ' +\n",
    "            product_df['store_clean'] + ' ' +\n",
    "            (product_df['main_category_clean'] + ' ') * 3 +  \n",
    "            product_df['details_clean'] + ' ' +\n",
    "            product_df['review_titles_clean'] + ' ' +\n",
    "            product_df['review_texts_clean']\n",
    "        )\n",
    "        \n",
    "        self.item_to_idx = {item: idx for idx, item in enumerate(product_df['parent_asin'])}\n",
    "        self.idx_to_item = {idx: item for item, idx in self.item_to_idx.items()}\n",
    "        \n",
    "        return product_df\n",
    "    \n",
    "    def fit(self, df, max_products=None): \n",
    "        print(\"Training Fast Content-Based Recommender...\")\n",
    "        \n",
    "        self.product_df = self.extract_product_features(df, max_products=max_products)\n",
    "        print(f\"Extracted features for {len(self.product_df)} unique products\")\n",
    "        \n",
    "        self.product_df = self.prepare_content_features(self.product_df)\n",
    "        \n",
    "        print(\"Creating TF-IDF features...\")\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,  \n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 1),  \n",
    "            min_df=5,\n",
    "            max_df=0.7\n",
    "        )\n",
    "        \n",
    "        tfidf_features = self.tfidf_vectorizer.fit_transform(self.product_df['combined_text'])\n",
    "        \n",
    "        print(\"Preparing numerical features...\")\n",
    "        numerical_features = self.product_df[['price', 'avg_rating', 'num_reviews', 'avg_helpful_votes']].fillna(0)\n",
    "        numerical_features_scaled = self.scaler.fit_transform(numerical_features)\n",
    "        numerical_sparse = csr_matrix(numerical_features_scaled)\n",
    "        \n",
    "        \n",
    "        self.content_features = hstack([\n",
    "            tfidf_features * 0.8, \n",
    "            numerical_sparse * 0.2\n",
    "        ])\n",
    "        \n",
    "        print(f\"Content features shape: {self.content_features.shape}\")\n",
    "        sparsity = 1 - (self.content_features.nnz / (self.content_features.shape[0] * self.content_features.shape[1]))\n",
    "        print(f\"Content features sparsity: {sparsity:.4f}\")\n",
    "        \n",
    "        del tfidf_features, numerical_features_scaled, numerical_sparse\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return self\n",
    "    \n",
    "    def precompute_similarities(self, top_k=50, batch_size=500, checkpoint_every=10000, \n",
    "                              resume_from=None, checkpoint_file=\"similarities_checkpoint.pkl\"):\n",
    "        print(f\"\\nPre-computing top-{top_k} similarities for {len(self.item_to_idx)} products...\")\n",
    "        print(\"CRASH RECOVERY enabled - progress will be saved!\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        n_items = self.content_features.shape[0]\n",
    "        \n",
    "        if resume_from and os.path.exists(checkpoint_file):\n",
    "            print(f\"Resuming from checkpoint: {checkpoint_file}\")\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "                self.item_similarities = checkpoint_data['item_similarities']\n",
    "                start_idx = checkpoint_data['last_processed'] + batch_size\n",
    "            print(f\"Resuming from product {start_idx}/{n_items}\")\n",
    "        else:\n",
    "            self.item_similarities = {}\n",
    "            start_idx = 0\n",
    "            print(\"Starting New computation...\")\n",
    "        \n",
    "        estimated_gb = (n_items * top_k * 32) / (1024**3)  \n",
    "        print(f\"Estimated memory needed: ~{estimated_gb:.1f} GB\")\n",
    "        if estimated_gb > 8:\n",
    "            print(\"WARNING: This may exceed your computer's memory!\")\n",
    "            print(\"Consider using max_products parameter to limit dataset size\")\n",
    "            \n",
    "        for current_idx in range(start_idx, n_items, batch_size):\n",
    "            end_idx = min(current_idx + batch_size, n_items)\n",
    "            \n",
    "            if current_idx % (checkpoint_every) == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                progress = current_idx / n_items * 100\n",
    "                eta = (elapsed / max(progress, 0.001)) * (100 - progress) if progress > 0 else 0\n",
    "                print(f\"Progress: {progress:.1f}% ({current_idx}/{n_items}) - ETA: {eta/60:.1f} min\")\n",
    "                \n",
    "                try:\n",
    "                    import psutil\n",
    "                    memory_percent = psutil.virtual_memory().percent\n",
    "                    if memory_percent > 85:\n",
    "                        print(f\"Memory usage high: {memory_percent:.1f}%\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            try:\n",
    "                batch_features = self.content_features[current_idx:end_idx]\n",
    "                \n",
    "                similarities = cosine_similarity(batch_features, self.content_features)\n",
    "                \n",
    "                for i, row_similarities in enumerate(similarities):\n",
    "                    actual_idx = current_idx + i\n",
    "                    item_id = self.idx_to_item[actual_idx]\n",
    "                    \n",
    "                    top_indices = np.argsort(row_similarities)[::-1]\n",
    "                    top_items = []\n",
    "                    \n",
    "                    for idx in top_indices:\n",
    "                        if idx != actual_idx and len(top_items) < top_k:\n",
    "                            similar_item_id = self.idx_to_item[idx]\n",
    "                            similarity_score = row_similarities[idx]\n",
    "                            if similarity_score > 0.01:  # Only store meaningful similarities\n",
    "                                top_items.append((similar_item_id, similarity_score))\n",
    "                    \n",
    "                    self.item_similarities[item_id] = top_items\n",
    "                \n",
    "                del similarities, batch_features\n",
    "                gc.collect()\n",
    "                \n",
    "            except MemoryError:\n",
    "                print(\" MEMORY ERROR! Try reducing batch_size or max_products\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\" ERROR at batch {current_idx}: {e}\")\n",
    "                break\n",
    "            \n",
    "            if current_idx % checkpoint_every == 0 and current_idx > 0:\n",
    "                checkpoint_data = {\n",
    "                    'item_similarities': self.item_similarities,\n",
    "                    'last_processed': current_idx + batch_size - 1,\n",
    "                    'total_items': n_items,\n",
    "                    'top_k': top_k\n",
    "                }\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(checkpoint_data, f)\n",
    "                print(f\"Checkpoint saved! (Processed {current_idx + batch_size}/{n_items})\")\n",
    "        \n",
    "        self.similarity_computed = True\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\" Similarity computation completed in {total_time/60:.1f} minutes!\")\n",
    "        print(f\" Average similarities per item: {np.mean([len(sims) for sims in self.item_similarities.values()]):.1f}\")\n",
    "        \n",
    "        if os.path.exists(checkpoint_file):\n",
    "            os.remove(checkpoint_file)\n",
    "            print(\"🗑  Checkpoint file cleaned up\")\n",
    "    \n",
    "    def get_user_recommendations(self, user_id, df, n_recommendations=10):\n",
    "        if not self.similarity_computed:\n",
    "            print(\"  Similarities not pre-computed! This will be slow...\")\n",
    "            print(\"Run model.precompute_similarities() first for fast recommendations\")\n",
    "            return self._slow_user_recommendations(user_id, df, n_recommendations)\n",
    "        \n",
    "        user_history = df[df['user_id'] == user_id]['parent_asin'].unique()\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            print(f\"No purchase history found for user: {user_id}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(user_history)} items in user history\")\n",
    "        \n",
    "        recommendation_scores = defaultdict(float)\n",
    "        \n",
    "        for purchased_item in user_history:\n",
    "            if purchased_item in self.item_similarities:\n",
    "                similar_items = self.item_similarities[purchased_item]\n",
    "                \n",
    "                for similar_item_id, similarity_score in similar_items:\n",
    "                    if similar_item_id not in user_history:\n",
    "                        recommendation_scores[similar_item_id] += similarity_score\n",
    "        \n",
    "        sorted_recommendations = sorted(\n",
    "            recommendation_scores.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return sorted_recommendations[:n_recommendations]\n",
    "    \n",
    "    def _slow_user_recommendations(self, user_id, df, n_recommendations=10):\n",
    "        user_history = df[df['user_id'] == user_id]['parent_asin'].unique()\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            return []\n",
    "        \n",
    "        recommendations = {}\n",
    "        batch_size = 1000\n",
    "        \n",
    "        for purchased_item in user_history[:5]: \n",
    "            if purchased_item in self.item_to_idx:\n",
    "                item_idx = self.item_to_idx[purchased_item]\n",
    "                item_features = self.content_features[item_idx:item_idx+1]\n",
    "                \n",
    "                n_items = self.content_features.shape[0]\n",
    "                \n",
    "                for start_idx in range(0, n_items, batch_size):\n",
    "                    end_idx = min(start_idx + batch_size, n_items)\n",
    "                    batch_features = self.content_features[start_idx:end_idx]\n",
    "                    \n",
    "                    similarities = cosine_similarity(item_features, batch_features).flatten()\n",
    "                    \n",
    "                    for i, similarity_score in enumerate(similarities):\n",
    "                        actual_idx = start_idx + i\n",
    "                        similar_item = self.idx_to_item[actual_idx]\n",
    "                        \n",
    "                        if similar_item not in user_history:\n",
    "                            if similar_item in recommendations:\n",
    "                                recommendations[similar_item] = max(recommendations[similar_item], similarity_score)\n",
    "                            else:\n",
    "                                recommendations[similar_item] = similarity_score\n",
    "        \n",
    "        sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_recommendations[:n_recommendations]\n",
    "    \n",
    "    def get_item_similarities(self, item_id, n_recommendations=10):\n",
    "        if not self.similarity_computed:\n",
    "            print(\"  Similarities not pre-computed!\")\n",
    "            return []\n",
    "            \n",
    "        if item_id not in self.item_similarities:\n",
    "            print(f\"Item {item_id} not found in similarity index\")\n",
    "            return []\n",
    "            \n",
    "        return self.item_similarities[item_id][:n_recommendations]\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        model_data = {\n",
    "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "            'content_features': self.content_features,\n",
    "            'item_to_idx': self.item_to_idx,\n",
    "            'idx_to_item': self.idx_to_item,\n",
    "            'scaler': self.scaler,\n",
    "            'product_df': self.product_df,\n",
    "            'item_similarities': self.item_similarities,\n",
    "            'similarity_computed': self.similarity_computed\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        \n",
    "        import os\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"File size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        self.tfidf_vectorizer = model_data['tfidf_vectorizer']\n",
    "        self.content_features = model_data['content_features']\n",
    "        self.item_to_idx = model_data['item_to_idx']\n",
    "        self.idx_to_item = model_data['idx_to_item']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.product_df = model_data['product_df']\n",
    "        self.item_similarities = model_data.get('item_similarities', {})\n",
    "        self.similarity_computed = model_data.get('similarity_computed', False)\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        if self.similarity_computed:\n",
    "            print(\"Pre-computed similarities loaded - recommendations will be FAST!\")\n",
    "        else:\n",
    "            print(\" No pre-computed similarities - run precompute_similarities() for speed\")\n",
    "\n"
   ],
   "id": "f41a35d549ecb474",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:49:20.387758Z",
     "start_time": "2025-09-22T22:49:20.357826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "class ChunkedContentRecommender:\n",
    "    def __init__(self, chunk_size=100000):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_models = []\n",
    "        self.chunk_files = []\n",
    "        self.total_products = 0\n",
    "        \n",
    "    def create_chunks_and_train(self, df, save_directory=\"chunk_models\"):\n",
    "        \n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "            \n",
    "        unique_products = df['parent_asin'].unique()\n",
    "        self.total_products = len(unique_products)\n",
    "        n_chunks = (self.total_products + self.chunk_size - 1) // self.chunk_size  \n",
    "        \n",
    "        print(f\" Creating {n_chunks} chunks for {self.total_products} products\")\n",
    "        print(f\" Chunk size: {self.chunk_size} products each\")\n",
    "        \n",
    "        chunk_info = []\n",
    "        \n",
    "        for chunk_num in range(n_chunks):\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\" PROCESSING CHUNK {chunk_num + 1}/{n_chunks}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            start_idx = chunk_num * self.chunk_size\n",
    "            end_idx = min(start_idx + self.chunk_size, self.total_products)\n",
    "            \n",
    "            chunk_products = unique_products[start_idx:end_idx]\n",
    "            chunk_df = df[df['parent_asin'].isin(chunk_products)]\n",
    "            \n",
    "            print(f\" Chunk {chunk_num + 1}: Products {start_idx} to {end_idx-1}\")\n",
    "            print(f\" Chunk contains {len(chunk_products)} unique products\")\n",
    "            print(f\" Chunk contains {len(chunk_df)} total reviews/records\")\n",
    "            \n",
    "            \n",
    "            from __main__ import FastContentBasedRecommender\n",
    "            \n",
    "            chunk_model = FastContentBasedRecommender()\n",
    "            \n",
    "            print(f\"  Training chunk {chunk_num + 1}...\")\n",
    "            chunk_model.fit(chunk_df)\n",
    "            \n",
    "            print(f\"⚡ Pre-computing similarities for chunk {chunk_num + 1}...\")\n",
    "            chunk_model.precompute_similarities(\n",
    "                top_k=50, \n",
    "                batch_size=500, \n",
    "                checkpoint_every=5000\n",
    "            )\n",
    "            \n",
    "            chunk_filename = f\"{save_directory}/chunk_{chunk_num:03d}.pkl\"\n",
    "            chunk_model.save_model(chunk_filename)\n",
    "            \n",
    "            chunk_info.append({\n",
    "                'chunk_id': chunk_num,\n",
    "                'filename': chunk_filename,\n",
    "                'product_range': (start_idx, end_idx),\n",
    "                'n_products': len(chunk_products),\n",
    "                'products': chunk_products.tolist()  \n",
    "            })\n",
    "            \n",
    "            print(f\" Chunk {chunk_num + 1} completed and saved!\")\n",
    "            \n",
    "            del chunk_model, chunk_df\n",
    "            gc.collect()\n",
    "        \n",
    "        metadata_file = f\"{save_directory}/chunk_metadata.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'total_products': self.total_products,\n",
    "                'chunk_size': self.chunk_size,\n",
    "                'n_chunks': n_chunks,\n",
    "                'chunks': chunk_info\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n ALL CHUNKS COMPLETED!\")\n",
    "        print(f\" Models saved in: {save_directory}/\")\n",
    "        print(f\"Metadata saved: {metadata_file}\")\n",
    "        \n",
    "        return chunk_info\n",
    "    \n",
    "    def load_chunks(self, save_directory=\"chunk_models\"):\n",
    "        metadata_file = f\"{save_directory}/chunk_metadata.json\"\n",
    "        \n",
    "        if not os.path.exists(metadata_file):\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {metadata_file}\")\n",
    "        \n",
    "        with open(metadata_file, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        print(f\" Loading {self.metadata['n_chunks']} chunk models...\")\n",
    "        \n",
    "        self.chunk_models = []\n",
    "        self.product_to_chunk = {}  \n",
    "        \n",
    "        from __main__ import FastContentBasedRecommender\n",
    "        \n",
    "        for chunk_info in self.metadata['chunks']:\n",
    "            chunk_id = chunk_info['chunk_id']\n",
    "            chunk_filename = chunk_info['filename']\n",
    "            \n",
    "            print(f\" Loading chunk {chunk_id + 1}...\")\n",
    "            \n",
    "            chunk_model = FastContentBasedRecommender()\n",
    "            chunk_model.load_model(chunk_filename)\n",
    "            self.chunk_models.append(chunk_model)\n",
    "            \n",
    "            for product_id in chunk_info['products']:\n",
    "                self.product_to_chunk[product_id] = chunk_id\n",
    "        \n",
    "        print(f\"All chunks loaded! Ready for recommendations.\")\n",
    "        return self.chunk_models\n",
    "    \n",
    "    def get_user_recommendations(self, user_id, df, n_recommendations=10, max_user_history=20):\n",
    "        \n",
    "        if not self.chunk_models:\n",
    "            raise ValueError(\"No chunks loaded! Call load_chunks() first.\")\n",
    "        \n",
    "        user_history = df[df['user_id'] == user_id]['parent_asin'].unique()\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            print(f\"No purchase history found for user: {user_id}\")\n",
    "            return []\n",
    "        \n",
    "        if len(user_history) > max_user_history:\n",
    "            user_history = user_history[-max_user_history:]  \n",
    "        \n",
    "        print(f\" Searching recommendations across {len(self.chunk_models)} chunks...\")\n",
    "        print(f\" User has {len(user_history)} items in recent history\")\n",
    "        \n",
    "        all_recommendations = defaultdict(float)\n",
    "        chunks_searched = 0\n",
    "        \n",
    "        for purchased_item in user_history:\n",
    "            if purchased_item in self.product_to_chunk:\n",
    "                chunk_idx = self.product_to_chunk[purchased_item]\n",
    "                chunk_model = self.chunk_models[chunk_idx]\n",
    "                chunks_searched += 1\n",
    "                \n",
    "                similar_items = chunk_model.get_item_similarities(purchased_item, n_recommendations=50)\n",
    "                \n",
    "                for similar_item, similarity_score in similar_items:\n",
    "                    if similar_item not in user_history:\n",
    "                        all_recommendations[similar_item] += similarity_score\n",
    "        \n",
    "        print(f\"Searched {chunks_searched} relevant chunks\")\n",
    "        \n",
    "        sorted_recommendations = sorted(\n",
    "            all_recommendations.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return sorted_recommendations[:n_recommendations]\n",
    "    \n",
    "    def get_item_similarities(self, item_id, n_recommendations=10):\n",
    "        \n",
    "        if not self.chunk_models:\n",
    "            raise ValueError(\"No chunks loaded! Call load_chunks() first.\")\n",
    "        \n",
    "        if item_id not in self.product_to_chunk:\n",
    "            print(f\"Item {item_id} not found in any chunk\")\n",
    "            return []\n",
    "        \n",
    "        chunk_idx = self.product_to_chunk[item_id]\n",
    "        chunk_model = self.chunk_models[chunk_idx]\n",
    "        \n",
    "        print(f\"Finding similarities in chunk {chunk_idx + 1}\")\n",
    "        \n",
    "        return chunk_model.get_item_similarities(item_id, n_recommendations)\n",
    "    \n",
    "    def get_system_stats(self):\n",
    "        if not hasattr(self, 'metadata'):\n",
    "            return \"No chunks loaded\"\n",
    "        \n",
    "        stats = {\n",
    "            'total_products': self.metadata['total_products'],\n",
    "            'n_chunks': self.metadata['n_chunks'],\n",
    "            'chunk_size': self.metadata['chunk_size'],\n",
    "            'chunks_loaded': len(self.chunk_models),\n",
    "            'memory_per_chunk_mb': 'Unknown'  \n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "\n",
    "def estimate_chunked_system_requirements(total_products, chunk_size=100000):\n",
    "    n_chunks = (total_products + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"CHUNKED SYSTEM ESTIMATES for {total_products:,} products:\")\n",
    "    print(f\"Number of chunks: {n_chunks}\")\n",
    "    print(f\"Products per chunk: {chunk_size:,}\")\n",
    "    print(f\"Training time per chunk: ~30-45 minutes\")\n",
    "    print(f\"Total training time: ~{n_chunks * 35 / 60:.1f} hours\")\n",
    "    print(f\"Memory per chunk: ~500MB - 1GB\")\n",
    "    print(f\"Total storage: ~{n_chunks * 0.8:.1f}GB\")\n",
    "    print(f\"Recommendation speed: ~0.01-0.1 seconds\")\n",
    "    \n",
    "    return n_chunks\n",
    "\n"
   ],
   "id": "9cff42f36f505f29",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T09:20:42.972228Z",
     "start_time": "2025-09-20T22:40:44.160938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunked_system = ChunkedContentRecommender(chunk_size=100000)\n",
    "\n",
    "\n",
    "chunk_info = chunked_system.create_chunks_and_train(df, save_directory=\"chunk_models\")"
   ],
   "id": "771ae068811138a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating 14 chunks for 1300847 products\n",
      " Chunk size: 100000 products each\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 1/14\n",
      "============================================================\n",
      " Chunk 1: Products 0 to 99999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 201366 total reviews/records\n",
      "  Training chunk 1...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9599\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 1...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "Memory usage high: 93.6%\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 30.4 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 28.0 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 26.8 min\n",
      "Memory usage high: 88.4%\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 25.9 min\n",
      "Memory usage high: 88.6%\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 24.2 min\n",
      "Memory usage high: 86.9%\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 22.5 min\n",
      "Memory usage high: 86.0%\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 20.8 min\n",
      "Memory usage high: 85.9%\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.2 min\n",
      "Memory usage high: 87.1%\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 17.5 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 15.8 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 14.3 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 12.7 min\n",
      "Memory usage high: 85.8%\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.2 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 9.6 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.0 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.5 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 4.9 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.3 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.6 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 32.7 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_000.pkl\n",
      "File size: 375.0 MB\n",
      " Chunk 1 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 2/14\n",
      "============================================================\n",
      " Chunk 2: Products 100000 to 199999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 158689 total reviews/records\n",
      "  Training chunk 2...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9516\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 2...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 29.5 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 28.6 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 27.1 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 25.7 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 24.1 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 22.5 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 21.0 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.5 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 18.0 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.5 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 14.9 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.3 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.6 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.0 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.3 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.7 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.0 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.4 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.7 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 33.9 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_001.pkl\n",
      "File size: 478.1 MB\n",
      " Chunk 2 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 3/14\n",
      "============================================================\n",
      " Chunk 3: Products 200000 to 299999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 225620 total reviews/records\n",
      "  Training chunk 3...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9492\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 3...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 31.2 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 29.5 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 27.9 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 26.2 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 24.6 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 23.0 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 21.5 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.9 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 18.3 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.8 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 15.1 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.5 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.8 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.2 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.5 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.8 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.2 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.5 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.8 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 35.2 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_002.pkl\n",
      "File size: 486.5 MB\n",
      " Chunk 3 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 4/14\n",
      "============================================================\n",
      " Chunk 4: Products 300000 to 399999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 166635 total reviews/records\n",
      "  Training chunk 4...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9291\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 4...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "Memory usage high: 86.8%\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 33.4 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 32.4 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 30.3 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 28.4 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 26.7 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 25.4 min\n",
      "Memory usage high: 85.3%\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 23.7 min\n",
      "Memory usage high: 85.1%\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 22.3 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 20.3 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 18.3 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 16.3 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 14.5 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 12.7 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.9 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 9.1 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 7.2 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.4 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.6 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.8 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 36.0 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_003.pkl\n",
      "File size: 874.1 MB\n",
      " Chunk 4 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 5/14\n",
      "============================================================\n",
      " Chunk 5: Products 400000 to 499999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 207223 total reviews/records\n",
      "  Training chunk 5...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9471\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 5...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "Memory usage high: 86.2%\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 31.6 min\n",
      "Memory usage high: 85.6%\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 29.8 min\n",
      "Memory usage high: 86.5%\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 28.2 min\n",
      "Memory usage high: 86.1%\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 26.5 min\n",
      "Memory usage high: 86.5%\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 24.8 min\n",
      "Memory usage high: 86.4%\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 23.2 min\n",
      "Memory usage high: 85.3%\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 21.6 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 20.0 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 18.4 min\n",
      "Memory usage high: 86.9%\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.8 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 15.0 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.3 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.6 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.0 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.3 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.7 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.0 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.3 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.7 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 33.4 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_004.pkl\n",
      "File size: 446.3 MB\n",
      " Chunk 5 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 6/14\n",
      "============================================================\n",
      " Chunk 6: Products 500000 to 599999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 182962 total reviews/records\n",
      "  Training chunk 6...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9554\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 6...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 28.1 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 28.7 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 27.1 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 25.5 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 23.9 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 22.3 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 20.7 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.3 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 17.8 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.3 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 14.7 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.1 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.5 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 9.9 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.3 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.6 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.0 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.3 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.7 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 33.5 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_005.pkl\n",
      "File size: 424.8 MB\n",
      " Chunk 6 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 7/14\n",
      "============================================================\n",
      " Chunk 7: Products 600000 to 699999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 207932 total reviews/records\n",
      "  Training chunk 7...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9519\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 7...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 29.8 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 28.6 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 27.2 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 25.6 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 24.0 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 22.6 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 20.9 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.3 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 17.8 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.3 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 14.7 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.2 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.6 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.0 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.3 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.7 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.0 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.4 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.7 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 33.8 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_006.pkl\n",
      "File size: 446.2 MB\n",
      " Chunk 7 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 8/14\n",
      "============================================================\n",
      " Chunk 8: Products 700000 to 799999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 154862 total reviews/records\n",
      "  Training chunk 8...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9457\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 8...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "Memory usage high: 85.6%\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 37.3 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 34.0 min\n",
      "Memory usage high: 85.8%\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 32.4 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 30.1 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 28.1 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 25.8 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 23.7 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 21.9 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 20.0 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 18.3 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 16.7 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 14.9 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 13.0 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 11.1 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 9.2 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 7.3 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.5 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.7 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.8 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 36.4 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_007.pkl\n",
      "File size: 490.2 MB\n",
      " Chunk 8 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 9/14\n",
      "============================================================\n",
      " Chunk 9: Products 800000 to 899999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 174335 total reviews/records\n",
      "  Training chunk 9...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9327\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 9...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "Memory usage high: 87.1%\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 31.5 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 30.2 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 28.5 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 26.9 min\n",
      "Memory usage high: 86.9%\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 25.4 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 23.7 min\n",
      "Memory usage high: 85.9%\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 22.1 min\n",
      "Memory usage high: 86.0%\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 20.4 min\n",
      "Memory usage high: 85.7%\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 18.6 min\n",
      "Memory usage high: 86.2%\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.9 min\n",
      "Memory usage high: 85.8%\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 15.2 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.5 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.8 min\n",
      "Memory usage high: 85.8%\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.2 min\n",
      "Memory usage high: 86.7%\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.5 min\n",
      "Memory usage high: 86.3%\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.8 min\n",
      "Memory usage high: 88.3%\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.1 min\n",
      "Memory usage high: 87.7%\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.4 min\n",
      "Memory usage high: 88.6%\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.7 min\n",
      "Memory usage high: 87.3%\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 34.2 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_008.pkl\n",
      "File size: 722.1 MB\n",
      " Chunk 9 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 10/14\n",
      "============================================================\n",
      " Chunk 10: Products 900000 to 999999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 201313 total reviews/records\n",
      "  Training chunk 10...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9480\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 10...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "Memory usage high: 88.4%\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 30.4 min\n",
      "Memory usage high: 88.6%\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 28.9 min\n",
      "Memory usage high: 87.3%\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 27.4 min\n",
      "Memory usage high: 87.1%\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 25.8 min\n",
      "Memory usage high: 87.3%\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 24.2 min\n",
      "Memory usage high: 87.0%\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 22.6 min\n",
      "Memory usage high: 86.8%\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 21.0 min\n",
      "Memory usage high: 87.4%\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.5 min\n",
      "Memory usage high: 86.6%\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 17.9 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 16.4 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 14.8 min\n",
      "Memory usage high: 86.5%\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.2 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.6 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.0 min\n",
      "Memory usage high: 85.3%\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.3 min\n",
      "Memory usage high: 85.3%\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.7 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.0 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.4 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.7 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 33.8 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_009.pkl\n",
      "File size: 495.5 MB\n",
      " Chunk 10 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 11/14\n",
      "============================================================\n",
      " Chunk 11: Products 1000000 to 1099999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 233725 total reviews/records\n",
      "  Training chunk 11...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9453\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 11...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 30.3 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 28.7 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 26.9 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 25.3 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 23.7 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 22.1 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 20.6 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 19.0 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 17.4 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 15.9 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 14.3 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 12.7 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 11.2 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 9.6 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.1 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 6.5 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 4.9 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.3 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.6 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 32.8 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_010.pkl\n",
      "File size: 511.4 MB\n",
      " Chunk 11 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 12/14\n",
      "============================================================\n",
      " Chunk 12: Products 1100000 to 1199999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 176926 total reviews/records\n",
      "  Training chunk 12...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9400\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 12...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 37.4 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 33.1 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 30.1 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 27.7 min\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 25.8 min\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 23.9 min\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 22.3 min\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 20.7 min\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 19.0 min\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 17.3 min\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 15.6 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.9 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 12.2 min\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.5 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.8 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 7.0 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.3 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.5 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.8 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 35.5 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_011.pkl\n",
      "File size: 552.0 MB\n",
      " Chunk 12 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 13/14\n",
      "============================================================\n",
      " Chunk 13: Products 1200000 to 1299999\n",
      " Chunk contains 100000 unique products\n",
      " Chunk contains 190925 total reviews/records\n",
      "  Training chunk 13...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 100000\n",
      "Processing product 1/100000 (0.0%)\n",
      "Processing product 50001/100000 (50.0%)\n",
      "Extracted features for 100000 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (100000, 1004)\n",
      "Content features sparsity: 0.9486\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 13...\n",
      "\n",
      "Pre-computing top-50 similarities for 100000 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.1 GB\n",
      "🔄 Progress: 0.0% (0/100000) - ETA: 0.0 min\n",
      "🔄 Progress: 5.0% (5000/100000) - ETA: 30.8 min\n",
      "Checkpoint saved! (Processed 5500/100000)\n",
      "🔄 Progress: 10.0% (10000/100000) - ETA: 29.6 min\n",
      "Checkpoint saved! (Processed 10500/100000)\n",
      "🔄 Progress: 15.0% (15000/100000) - ETA: 28.2 min\n",
      "Checkpoint saved! (Processed 15500/100000)\n",
      "🔄 Progress: 20.0% (20000/100000) - ETA: 27.3 min\n",
      "Memory usage high: 88.2%\n",
      "Checkpoint saved! (Processed 20500/100000)\n",
      "🔄 Progress: 25.0% (25000/100000) - ETA: 25.6 min\n",
      "Memory usage high: 87.9%\n",
      "Checkpoint saved! (Processed 25500/100000)\n",
      "🔄 Progress: 30.0% (30000/100000) - ETA: 24.0 min\n",
      "Memory usage high: 85.5%\n",
      "Checkpoint saved! (Processed 30500/100000)\n",
      "🔄 Progress: 35.0% (35000/100000) - ETA: 22.3 min\n",
      "Memory usage high: 85.8%\n",
      "Checkpoint saved! (Processed 35500/100000)\n",
      "🔄 Progress: 40.0% (40000/100000) - ETA: 20.6 min\n",
      "Memory usage high: 86.3%\n",
      "Checkpoint saved! (Processed 40500/100000)\n",
      "🔄 Progress: 45.0% (45000/100000) - ETA: 18.9 min\n",
      "Memory usage high: 86.5%\n",
      "Checkpoint saved! (Processed 45500/100000)\n",
      "🔄 Progress: 50.0% (50000/100000) - ETA: 17.2 min\n",
      "Memory usage high: 86.7%\n",
      "Checkpoint saved! (Processed 50500/100000)\n",
      "🔄 Progress: 55.0% (55000/100000) - ETA: 15.5 min\n",
      "Checkpoint saved! (Processed 55500/100000)\n",
      "🔄 Progress: 60.0% (60000/100000) - ETA: 13.8 min\n",
      "Checkpoint saved! (Processed 60500/100000)\n",
      "🔄 Progress: 65.0% (65000/100000) - ETA: 12.1 min\n",
      "Memory usage high: 85.2%\n",
      "Checkpoint saved! (Processed 65500/100000)\n",
      "🔄 Progress: 70.0% (70000/100000) - ETA: 10.6 min\n",
      "Checkpoint saved! (Processed 70500/100000)\n",
      "🔄 Progress: 75.0% (75000/100000) - ETA: 8.9 min\n",
      "Checkpoint saved! (Processed 75500/100000)\n",
      "🔄 Progress: 80.0% (80000/100000) - ETA: 7.2 min\n",
      "Checkpoint saved! (Processed 80500/100000)\n",
      "🔄 Progress: 85.0% (85000/100000) - ETA: 5.4 min\n",
      "Checkpoint saved! (Processed 85500/100000)\n",
      "🔄 Progress: 90.0% (90000/100000) - ETA: 3.6 min\n",
      "Checkpoint saved! (Processed 90500/100000)\n",
      "🔄 Progress: 95.0% (95000/100000) - ETA: 1.8 min\n",
      "Checkpoint saved! (Processed 95500/100000)\n",
      " Similarity computation completed in 36.0 minutes!\n",
      " Average similarities per item: 50.0\n",
      "🗑  Checkpoint file cleaned up\n",
      "Model saved to chunk_models/chunk_012.pkl\n",
      "File size: 499.0 MB\n",
      " Chunk 13 completed and saved!\n",
      "\n",
      "============================================================\n",
      " PROCESSING CHUNK 14/14\n",
      "============================================================\n",
      " Chunk 14: Products 1300000 to 1300846\n",
      " Chunk contains 847 unique products\n",
      " Chunk contains 887 total reviews/records\n",
      "  Training chunk 14...\n",
      "Training Fast Content-Based Recommender...\n",
      "Extracting features for products...\n",
      "Total unique products in dataset: 847\n",
      "Processing product 1/847 (0.0%)\n",
      "Extracted features for 847 unique products\n",
      "Preparing content features...\n",
      "Creating TF-IDF features...\n",
      "Preparing numerical features...\n",
      "Content features shape: (847, 1004)\n",
      "Content features sparsity: 0.9646\n",
      "Training completed!\n",
      "⚡ Pre-computing similarities for chunk 14...\n",
      "\n",
      "Pre-computing top-50 similarities for 847 products...\n",
      "✅ CRASH RECOVERY enabled - progress will be saved!\n",
      "Starting New computation...\n",
      "Estimated memory needed: ~0.0 GB\n",
      "🔄 Progress: 0.0% (0/847) - ETA: 0.0 min\n",
      " Similarity computation completed in 0.0 minutes!\n",
      " Average similarities per item: 50.0\n",
      "Model saved to chunk_models/chunk_013.pkl\n",
      "File size: 3.0 MB\n",
      " Chunk 14 completed and saved!\n",
      "\n",
      " ALL CHUNKS COMPLETED!\n",
      " Models saved in: chunk_models/\n",
      "Metadata saved: chunk_models/chunk_metadata.json\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eba42922ed463408"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:45:31.986616Z",
     "start_time": "2025-09-22T22:44:21.695242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunked_system = ChunkedContentRecommender()\n",
    "chunked_system.load_chunks(\"chunk_models\")"
   ],
   "id": "a4ba7eaad096f43a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading 14 chunk models...\n",
      " Loading chunk 1...\n",
      "Model loaded from chunk_models/chunk_000.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 2...\n",
      "Model loaded from chunk_models/chunk_001.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 3...\n",
      "Model loaded from chunk_models/chunk_002.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 4...\n",
      "Model loaded from chunk_models/chunk_003.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 5...\n",
      "Model loaded from chunk_models/chunk_004.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 6...\n",
      "Model loaded from chunk_models/chunk_005.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 7...\n",
      "Model loaded from chunk_models/chunk_006.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 8...\n",
      "Model loaded from chunk_models/chunk_007.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 9...\n",
      "Model loaded from chunk_models/chunk_008.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 10...\n",
      "Model loaded from chunk_models/chunk_009.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 11...\n",
      "Model loaded from chunk_models/chunk_010.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 12...\n",
      "Model loaded from chunk_models/chunk_011.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 13...\n",
      "Model loaded from chunk_models/chunk_012.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      " Loading chunk 14...\n",
      "Model loaded from chunk_models/chunk_013.pkl\n",
      "Pre-computed similarities loaded - recommendations will be FAST!\n",
      "All chunks loaded! Ready for recommendations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.FastContentBasedRecommender at 0x22b7d5bab10>,\n",
       " <__main__.FastContentBasedRecommender at 0x22b9fbb13d0>,\n",
       " <__main__.FastContentBasedRecommender at 0x22b8cb0b350>,\n",
       " <__main__.FastContentBasedRecommender at 0x22e1e974710>,\n",
       " <__main__.FastContentBasedRecommender at 0x22e5bc08990>,\n",
       " <__main__.FastContentBasedRecommender at 0x22e9a2d8790>,\n",
       " <__main__.FastContentBasedRecommender at 0x22ef732cad0>,\n",
       " <__main__.FastContentBasedRecommender at 0x22f32e2c6d0>,\n",
       " <__main__.FastContentBasedRecommender at 0x22f6d564990>,\n",
       " <__main__.FastContentBasedRecommender at 0x22fa9480b10>,\n",
       " <__main__.FastContentBasedRecommender at 0x22fe80a46d0>,\n",
       " <__main__.FastContentBasedRecommender at 0x230378a8d10>,\n",
       " <__main__.FastContentBasedRecommender at 0x230769c0b10>,\n",
       " <__main__.FastContentBasedRecommender at 0x230b6aa4910>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:49:25.596156Z",
     "start_time": "2025-09-22T22:49:25.236081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "recommendations = chunked_system.get_user_recommendations(\n",
    "    'AGZZXSMMS4WRHHJRBUJZI4FZDHKQ', \n",
    "    df, \n",
    "    n_recommendations=10\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Recommendation time: {end_time - start_time:.4f} seconds\")\n",
    "print(\"\\nChunked System Recommendations:\")\n",
    "for i, (item_id, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. Product: {item_id} - Score: {score:.4f}\")"
   ],
   "id": "5c1c0a5e6a258db7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Searching recommendations across 14 chunks...\n",
      " User has 20 items in recent history\n",
      "Searched 20 relevant chunks\n",
      "Recommendation time: 0.3472 seconds\n",
      "\n",
      "Chunked System Recommendations:\n",
      "1. Product: B07Z3LG2NW - Score: 2.3272\n",
      "2. Product: B082ZY7LTY - Score: 1.3555\n",
      "3. Product: B01A885HVA - Score: 1.2938\n",
      "4. Product: B0814NWRT4 - Score: 1.2729\n",
      "5. Product: B09229VMXD - Score: 1.2223\n",
      "6. Product: B07XV75LHK - Score: 1.2093\n",
      "7. Product: B07Z3NN94Z - Score: 1.1822\n",
      "8. Product: B08WFBN4WS - Score: 1.1807\n",
      "9. Product: B09XQ2CYQD - Score: 1.1583\n",
      "10. Product: B07YFKQ664 - Score: 1.1144\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:49:33.159218Z",
     "start_time": "2025-09-22T22:49:33.149487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similar_items = chunked_system.get_item_similarities('B07B4JXK8D', n_recommendations=10)\n",
    "\n",
    "print(\"\\nSimilar Items:\")\n",
    "for i, (item_id, score) in enumerate(similar_items, 1):\n",
    "    print(f\"{i}. {item_id} - Similarity: {score:.4f}\")"
   ],
   "id": "54eb81a3d0f516d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similarities in chunk 1\n",
      "\n",
      "Similar Items:\n",
      "1. B01N0TQ0OH - Similarity: 0.9971\n",
      "2. B07WTXWC32 - Similarity: 0.9970\n",
      "3. B0B3DB5HTC - Similarity: 0.9962\n",
      "4. B081KSD3BK - Similarity: 0.9960\n",
      "5. B00UXG4WR8 - Similarity: 0.9958\n",
      "6. B07TK15BQQ - Similarity: 0.9952\n",
      "7. B07SM1PGJW - Similarity: 0.9952\n",
      "8. B07RNJY499 - Similarity: 0.9932\n",
      "9. B081K4Q6KQ - Similarity: 0.9917\n",
      "10. B092LLM7H3 - Similarity: 0.9917\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:50:05.422019Z",
     "start_time": "2025-09-22T22:50:05.408019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class EnsembleRecommender:\n",
    "    def __init__(self, content_model=None, collaborative_model=None):\n",
    "        self.content_model = content_model\n",
    "        self.collaborative_model = collaborative_model\n",
    "        \n",
    "    def set_content_model(self, content_model):\n",
    "        self.content_model = content_model\n",
    "        \n",
    "    def set_collaborative_model(self, collaborative_model):\n",
    "        self.collaborative_model = collaborative_model\n",
    "        \n",
    "    def get_ensemble_recommendations(self, user_id, df, n_recommendations=10, \n",
    "                                   content_weight=0.6, collaborative_weight=0.4):\n",
    "        print(f\"Getting ensemble recommendations for user: {user_id}\")\n",
    "        \n",
    "        all_recommendations = defaultdict(float)\n",
    "        \n",
    "        if self.content_model:\n",
    "            try:\n",
    "                content_recs = self.content_model.get_user_recommendations(\n",
    "                    user_id, df, n_recommendations=20\n",
    "                )\n",
    "                \n",
    "                if content_recs:\n",
    "                    max_score = max([score for _, score in content_recs])\n",
    "                    for item_id, score in content_recs:\n",
    "                        normalized_score = score / max_score if max_score > 0 else 0\n",
    "                        all_recommendations[item_id] += normalized_score * content_weight\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Content model error: {e}\")\n",
    "        \n",
    "        if self.collaborative_model:\n",
    "            try:\n",
    "                collaborative_recs = self.collaborative_model.get_user_recommendations(\n",
    "                    user_id, df, n_recommendations=20\n",
    "                )\n",
    "                \n",
    "                if collaborative_recs:\n",
    "                    max_score = max([score for _, score in collaborative_recs])\n",
    "                    for item_id, score in collaborative_recs:\n",
    "                        normalized_score = score / max_score if max_score > 0 else 0\n",
    "                        all_recommendations[item_id] += normalized_score * collaborative_weight\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Collaborative model error: {e}\")\n",
    "        \n",
    "        sorted_recommendations = sorted(\n",
    "            all_recommendations.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return sorted_recommendations[:n_recommendations]\n",
    "\n",
    "class CollaborativeModelWrapper:\n",
    "    def __init__(self, mf_model, user_to_idx, idx_to_item, interaction_matrix, get_smart_recommendations_func):\n",
    "        self.mf_model = mf_model\n",
    "        self.user_to_idx = user_to_idx\n",
    "        self.idx_to_item = idx_to_item\n",
    "        self.interaction_matrix = interaction_matrix\n",
    "        self.get_recommendations_func = get_smart_recommendations_func\n",
    "    \n",
    "    def get_user_recommendations(self, user_id, df, n_recommendations=10):\n",
    "        recommendations = self.get_recommendations_func(\n",
    "            user_id, df, self.mf_model, self.user_to_idx, \n",
    "            self.idx_to_item, self.interaction_matrix, n_recommendations\n",
    "        )\n",
    "        \n",
    "        result = []\n",
    "        for rec in recommendations:\n",
    "            if len(rec) == 3:\n",
    "                item_id, score, category = rec\n",
    "                result.append((item_id, score))\n",
    "            else:\n",
    "                result.append(rec)\n",
    "        return result"
   ],
   "id": "2ffe4fccdeb49636",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T22:50:28.313209Z",
     "start_time": "2025-09-22T22:50:08.373822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ensemble = EnsembleRecommender()\n",
    "\n",
    "collab_wrapper = CollaborativeModelWrapper(\n",
    "    mf_model, user_to_idx, idx_to_item, interaction_matrix, get_smart_recommendations\n",
    ")\n",
    "\n",
    "ensemble.set_content_model(chunked_system)\n",
    "ensemble.set_collaborative_model(collab_wrapper)\n",
    "\n",
    "sample_users = df['user_id'].value_counts().head(5).index.tolist()\n",
    "test_user = sample_users[0]\n",
    "\n",
    "ensemble_recs = ensemble.get_ensemble_recommendations(test_user, df, n_recommendations=10)\n",
    "\n",
    "for i, (item_id, score) in enumerate(ensemble_recs, 1):\n",
    "    item_info = df[df['parent_asin'] == item_id].iloc[0]\n",
    "    print(f\"{i}. {item_info['title_meta'][:50]}... (Score: {score:.3f})\")"
   ],
   "id": "97ddef254e5c7ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ensemble recommendations for user: AGZZXSMMS4WRHHJRBUJZI4FZDHKQ\n",
      " Searching recommendations across 14 chunks...\n",
      " User has 20 items in recent history\n",
      "Searched 20 relevant chunks\n",
      "User history categories: main_category\n",
      "AMAZON FASHION               3498\n",
      "Toys & Games                 1676\n",
      "Amazon Home                  1267\n",
      "Office Products               619\n",
      "Tools & Home Improvement      496\n",
      "Arts, Crafts & Sewing         298\n",
      "All Electronics               283\n",
      "All Beauty                    252\n",
      "Books                         189\n",
      "Sports & Outdoors             173\n",
      "Industrial & Scientific       111\n",
      "Health & Personal Care         92\n",
      "Musical Instruments            80\n",
      "Computers                      74\n",
      "Pet Supplies                   58\n",
      "Cell Phones & Accessories      42\n",
      "Camera & Photo                 39\n",
      "Grocery                        36\n",
      "Automotive                     20\n",
      "Baby                           17\n",
      "                               17\n",
      "Handmade                       13\n",
      "Home Audio & Theater           10\n",
      "Video Games                    10\n",
      "Car Electronics                 9\n",
      "Premium Beauty                  6\n",
      "Software                        3\n",
      "Buy a Kindle                    2\n",
      "Appliances                      2\n",
      "Fine Art                        1\n",
      "Movies & TV                     1\n",
      "Name: count, dtype: int64\n",
      "Detected primary category: AMAZON FASHION\n",
      "Low collaborative confidence for AGZZXSMMS4WRHHJRBUJZI4FZDHKQ, using category fallback\n",
      "1. Coopay 540 Pieces Fall Leaf Stickers Thanksgiving ... (Score: 0.600)\n",
      "2. Funny Civil Engineers TShirt I'm A Crazy Civil Eng... (Score: 0.400)\n",
      "3. DASH Rapid Egg Cooker: 6 Egg Capacity Electric Egg... (Score: 0.378)\n",
      "4. 12 Pack Keurig Filter Replacement by K&J - Compati... (Score: 0.371)\n",
      "5. Alvada Merino Wool Hiking Socks Thermal Warm Crew ... (Score: 0.368)\n",
      "6. Linda's Essentials Silicone Stove Gap Covers (2 Pa... (Score: 0.350)\n",
      "7. Tyuobox Gaming Headset for PS4, PC, Xbox One Contr... (Score: 0.349)\n",
      "8. Cimkiz Dishwasher Magnet Clean Dirty Sign Shutter ... (Score: 0.335)\n",
      "9. DND Miniatures Condition Markers - 96 Rings to Tra... (Score: 0.334)\n",
      "10. 2-Item Bundle: 6-Pack Cafe Save Reusable K Cup Cof... (Score: 0.329)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef619182d8bb524c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
